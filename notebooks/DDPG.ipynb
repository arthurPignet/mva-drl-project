{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n",
    "The above imports are not at all mandatory in order to have the following code work. This cell is here to ensure that your setup is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent on pendulum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acme\n",
    "\n",
    "from src.environments.inverted_pendulum import InvertedPendulumEnv, inverted_pendulum_env_factory\n",
    "from src.agents import DDPGAgent\n",
    "from src.interaction_loops import ddpg_parallel_interaction_loop, evaluation_parallel_interaction_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum_environment = InvertedPendulumEnv(for_evaluation=True)  #### WARNING, I changed the env\n",
    "pendulum_environment_spec = acme.make_environment_spec(pendulum_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent\n",
    "num_evaluation_steps = 1000\n",
    "max_learner_steps=100\n",
    "pendulum_agent = DDPGAgent(seed=0, \n",
    "learning_rate=5e-4,\n",
    "gamma=.95, \n",
    "environment_spec=pendulum_environment_spec, tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration nb10\n",
      "actions_mean: 0.87\t|mean_reward: -7.80\t|obs: 0.37\t|policy_loss: 3.11\t|value_loss: 15.11\t|value_mean: -3.11\t|value_target_mean: -7.80\n",
      "iteration nb20\n",
      "actions_mean: 0.53\t|mean_reward: -5.32\t|obs: 0.15\t|policy_loss: 6.68\t|value_loss: 2.15\t|value_mean: -6.69\t|value_target_mean: -5.32\n",
      "iteration nb30\n",
      "actions_mean: 0.59\t|mean_reward: -5.58\t|obs: -0.02\t|policy_loss: 4.04\t|value_loss: 2.34\t|value_mean: -4.04\t|value_target_mean: -5.58\n",
      "iteration nb40\n",
      "actions_mean: 0.53\t|mean_reward: -6.90\t|obs: 0.16\t|policy_loss: 3.96\t|value_loss: 4.82\t|value_mean: -3.96\t|value_target_mean: -6.90\n",
      "iteration nb50\n",
      "actions_mean: 0.54\t|mean_reward: -7.40\t|obs: 0.01\t|policy_loss: 6.20\t|value_loss: 1.44\t|value_mean: -6.20\t|value_target_mean: -7.40\n",
      "iteration nb60\n",
      "actions_mean: 0.47\t|mean_reward: -7.19\t|obs: 0.03\t|policy_loss: 6.05\t|value_loss: 1.43\t|value_mean: -6.05\t|value_target_mean: -7.19\n",
      "iteration nb70\n",
      "actions_mean: 0.32\t|mean_reward: -7.38\t|obs: 0.29\t|policy_loss: 5.73\t|value_loss: 1.78\t|value_mean: -5.73\t|value_target_mean: -7.38\n",
      "iteration nb80\n",
      "actions_mean: 0.07\t|mean_reward: -7.17\t|obs: 0.02\t|policy_loss: 6.38\t|value_loss: 0.77\t|value_mean: -6.38\t|value_target_mean: -7.17\n",
      "iteration nb90\n",
      "actions_mean: 0.05\t|mean_reward: -7.57\t|obs: 0.04\t|policy_loss: 5.91\t|value_loss: 2.18\t|value_mean: -5.91\t|value_target_mean: -7.57\n"
     ]
    }
   ],
   "source": [
    "ddpg_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, max_learner_steps=max_learner_steps, num_actors=16, batch_size=32, buffer_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum_agent._learner_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_map(lambda x,y:x+y, pendulum_agent._learner_state.params, pendulum_agent._learner_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "  ddpg_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, max_learner_steps=max_learner_steps, sequence_length=sequence_length, num_actors=16)\n",
    "  plot_policy_on_pendulum(pendulum_agent, 50)\n",
    "  evaluation_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, sequence_length=200, num_actors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import ValueNetworkDDPG, PolicyNetworkDDPG, PolicyNetwork\n",
    "from src.data import Transition\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Transition\n",
    "observation = pendulum_environment_spec.observations.generate_value()\n",
    "action = pendulum_environment_spec.actions.generate_value()\n",
    "\n",
    "tr = Transition(\n",
    "    obs_tm1= observation[None],\n",
    "    action_tm1= action[None],\n",
    "    reward_t= jnp.zeros(1)[None],\n",
    "    discount_t=jnp.zeros(1)[None],\n",
    "    obs_t=observation[None],\n",
    "    done=jnp.zeros(1)[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.concatenate([tr.obs_t, tr.action_tm1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(transition):\n",
    "    action = PolicyNetworkDDPG(environment_spec.actions)(transition.obs_tm1, True)\n",
    "    return action\n",
    "\n",
    "def f1(obs):\n",
    "    return hk.BatchApply(PolicyNetworkDDPG(environment_spec.actions))(obs, True)\n",
    "    \n",
    "init_loss, apply_loss = hk.without_apply_rng(hk.transform_with_state(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loss(rng,tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.obs_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474ad13565867984240761aae35b74af33d0ec1a4a5a71bd466163dfcadb0220"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('drl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
