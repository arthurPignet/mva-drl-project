{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n",
    "The above imports are not at all mandatory in order to have the following code work. This cell is here to ensure that your setup is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arthur/drl/mva-drl-project\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent on pendulum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acme\n",
    "\n",
    "from src.environments.inverted_pendulum import InvertedPendulumEnv, inverted_pendulum_env_factory\n",
    "from src.agents import DDPGAgent\n",
    "from src.interaction_loops import ddpg_parallel_interaction_loop, evaluation_parallel_interaction_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum_environment = InvertedPendulumEnv(for_evaluation=True)  #### WARNING, I changed the env\n",
    "pendulum_environment_spec = acme.make_environment_spec(pendulum_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent\n",
    "max_learner_steps=100000\n",
    "pendulum_agent = DDPGAgent(seed=0, \n",
    "actor_learning_rate=5e-4,\n",
    "critic_learning_rate=5e-4 ,\n",
    "gamma=.95, \n",
    "environment_spec=pendulum_environment_spec, tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected dict, got None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/drl/mva-drl-project/notebooks/DDPG.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Barthur_syft/home/arthur/drl/mva-drl-project/notebooks/DDPG.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m ddpg_parallel_interaction_loop(pendulum_agent, env_factory\u001b[39m=\u001b[39;49minverted_pendulum_env_factory, max_learner_steps\u001b[39m=\u001b[39;49mmax_learner_steps, num_actors\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, buffer_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/drl/mva-drl-project/src/interaction_loops.py:126\u001b[0m, in \u001b[0;36mddpg_parallel_interaction_loop\u001b[0;34m(agent, env_factory, max_learner_steps, buffer_size, batch_size, num_actors, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/interaction_loops.py?line=123'>124</a>\u001b[0m   replay\u001b[39m.\u001b[39madd(timestep\u001b[39m.\u001b[39mobservation[i],actions[i],ts\u001b[39m.\u001b[39mreward[i],ts\u001b[39m.\u001b[39mdiscount[i],ts\u001b[39m.\u001b[39mobservation[i],ts\u001b[39m.\u001b[39mstep_type[i])\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/interaction_loops.py?line=124'>125</a>\u001b[0m transitions \u001b[39m=\u001b[39m replay\u001b[39m.\u001b[39msample_batch(\u001b[39mmin\u001b[39m(batch_size,\u001b[39mlen\u001b[39m(replay\u001b[39m.\u001b[39m_memory)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> <a href='file:///home/arthur/drl/mva-drl-project/src/interaction_loops.py?line=125'>126</a>\u001b[0m logs \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mlearner_step(transitions)\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/interaction_loops.py?line=126'>127</a>\u001b[0m \u001b[39mif\u001b[39;00m learner_step \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/interaction_loops.py?line=127'>128</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39miteration nb\u001b[39m\u001b[39m{\u001b[39;00mlearner_step\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/drl/mva-drl-project/src/agents.py:283\u001b[0m, in \u001b[0;36mDDPGAgent.learner_step\u001b[0;34m(self, transition)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=281'>282</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearner_step\u001b[39m(\u001b[39mself\u001b[39m, transition: Transition) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mapping[\u001b[39mstr\u001b[39m, chex\u001b[39m.\u001b[39mArrayNumpy]:\n\u001b[0;32m--> <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=282'>283</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learner_state, logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner_state, transition)\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=283'>284</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logs\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/drl/mva-drl-project/src/agents.py:273\u001b[0m, in \u001b[0;36mDDPGAgent._update_fn\u001b[0;34m(self, learner_state, transition)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=269'>270</a>\u001b[0m critic_udpates, new_opt_critic_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_critic_optimizer\u001b[39m.\u001b[39mupdate(critic_grads_only_value, learner_state\u001b[39m.\u001b[39mopt_critic_state, critic_params)\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=270'>271</a>\u001b[0m actor_updates, new_opt_actor_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actor_optimizer\u001b[39m.\u001b[39mupdate(actor_grads_only_policy, learner_state\u001b[39m.\u001b[39mopt_actor_state, actor_params)\n\u001b[0;32m--> <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=272'>273</a>\u001b[0m new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39;49mapply_updates(learner_state\u001b[39m.\u001b[39;49mparams, actor_updates\u001b[39m.\u001b[39;49mupdate(critic_udpates))\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=274'>275</a>\u001b[0m learner_state\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mupdate(new_params)\n\u001b[1;32m    <a href='file:///home/arthur/drl/mva-drl-project/src/agents.py?line=275'>276</a>\u001b[0m state\u001b[39m.\u001b[39mupdate(actor_state)\u001b[39m.\u001b[39mupdate(critic_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py:42\u001b[0m, in \u001b[0;36mapply_updates\u001b[0;34m(params, updates)\u001b[0m\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_updates\u001b[39m(params: base\u001b[39m.\u001b[39mParams, updates: base\u001b[39m.\u001b[39mUpdates) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m base\u001b[39m.\u001b[39mParams:\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=24'>25</a>\u001b[0m   \u001b[39m\"\"\"Applies an update to the corresponding parameters.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=26'>27</a>\u001b[0m \u001b[39m  This is a utility functions that applies an update to a set of parameters, and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=39'>40</a>\u001b[0m \u001b[39m    Updated parameters, with same structure, shape and type as `params`.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=40'>41</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=41'>42</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mtree_multimap(\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=42'>43</a>\u001b[0m       \u001b[39mlambda\u001b[39;49;00m p, u: jnp\u001b[39m.\u001b[39;49masarray(p \u001b[39m+\u001b[39;49m u)\u001b[39m.\u001b[39;49mastype(jnp\u001b[39m.\u001b[39;49masarray(p)\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m     <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/optax/_src/update.py?line=43'>44</a>\u001b[0m       params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py:179\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=157'>158</a>\u001b[0m \u001b[39m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=158'>159</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=159'>160</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=175'>176</a>\u001b[0m \u001b[39m  ``rest``.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=176'>177</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=177'>178</a>\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=178'>179</a>\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39;49mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[1;32m    <a href='file:///home/arthur/anaconda3/envs/drl/lib/python3.9/site-packages/jax/_src/tree_util.py?line=179'>180</a>\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39munflatten(f(\u001b[39m*\u001b[39mxs) \u001b[39mfor\u001b[39;00m xs \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected dict, got None."
     ]
    }
   ],
   "source": [
    "ddpg_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, max_learner_steps=max_learner_steps, num_actors=16, batch_size=32, buffer_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = pendulum_agent\n",
    "\n",
    "self._rng, init_rng = jax.random.split(self._rng)\n",
    "learner_state = self._init_fn(init_rng, self._generate_dummy_transition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['policy/batch_norm/~/mean_ema', 'policy/batch_norm/~/var_ema', 'policy/batch_norm_1/~/mean_ema', 'policy/batch_norm_1/~/var_ema', 'policy/batch_norm_2/~/mean_ema', 'policy/batch_norm_2/~/var_ema', 'policy_target/batch_norm/~/mean_ema', 'policy_target/batch_norm/~/var_ema', 'policy_target/batch_norm_1/~/mean_ema', 'policy_target/batch_norm_1/~/var_ema', 'policy_target/batch_norm_2/~/mean_ema', 'policy_target/batch_norm_2/~/var_ema', 'value/batch_norm/~/mean_ema', 'value/batch_norm/~/var_ema', 'value/batch_norm_1/~/mean_ema', 'value/batch_norm_1/~/var_ema', 'value_target/batch_norm/~/mean_ema', 'value_target/batch_norm/~/var_ema', 'value_target/batch_norm_1/~/mean_ema', 'value_target/batch_norm_1/~/var_ema'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pendulum_agent._learner_state.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_map(lambda x,y:x+y, pendulum_agent._learner_state.params, pendulum_agent._learner_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "  ddpg_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, max_learner_steps=max_learner_steps, sequence_length=sequence_length, num_actors=16)\n",
    "  plot_policy_on_pendulum(pendulum_agent, 50)\n",
    "  evaluation_parallel_interaction_loop(pendulum_agent, env_factory=inverted_pendulum_env_factory, sequence_length=200, num_actors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import ValueNetworkDDPG, PolicyNetworkDDPG, PolicyNetwork\n",
    "from src.data import Transition\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Transition\n",
    "observation = pendulum_environment_spec.observations.generate_value()\n",
    "action = pendulum_environment_spec.actions.generate_value()\n",
    "\n",
    "tr = Transition(\n",
    "    obs_tm1= observation[None],\n",
    "    action_tm1= action[None],\n",
    "    reward_t= jnp.zeros(1)[None],\n",
    "    discount_t=jnp.zeros(1)[None],\n",
    "    obs_t=observation[None],\n",
    "    done=jnp.zeros(1)[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.concatenate([tr.obs_t, tr.action_tm1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(transition):\n",
    "    action = PolicyNetworkDDPG(environment_spec.actions)(transition.obs_tm1, True)\n",
    "    return action\n",
    "\n",
    "def f1(obs):\n",
    "    return hk.BatchApply(PolicyNetworkDDPG(environment_spec.actions))(obs, True)\n",
    "    \n",
    "init_loss, apply_loss = hk.without_apply_rng(hk.transform_with_state(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loss(rng,tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.obs_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474ad13565867984240761aae35b74af33d0ec1a4a5a71bd466163dfcadb0220"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('drl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
